{"cells":[{"cell_type":"code","execution_count":2,"id":"5WpQcXmCbBdC","metadata":{"id":"5WpQcXmCbBdC"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA: True\n"]}],"source":["# !pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117\n","import torch\n","from torch import nn\n","import os\n","from os import path\n","import torchvision\n","import torchvision.transforms as T\n","from typing import Sequence\n","from torchvision.transforms import functional as F\n","import numbers\n","import random\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import torchmetrics as TM\n","from tqdm import tqdm\n","from typing import List, Tuple\n","from pathlib import Path\n","\n","# Convert a pytorch tensor into a PIL image\n","t2img = T.ToPILImage()\n","# Convert a PIL image into a pytorch tensor\n","img2t = T.ToTensor()\n","working_dir = Path(\"data/\")\n","models_dir = Path(\"models/\")\n","\n","# Validation: Check if CUDA is available\n","print(f\"CUDA: {torch.cuda.is_available()}\")"]},{"cell_type":"code","execution_count":3,"id":"dwKYWKRYoEw4","metadata":{"id":"dwKYWKRYoEw4"},"outputs":[],"source":["# Create a dataset wrapper that allows us to perform custom image augmentations\n","# on both the target and label (segmentation mask) images.\n","#\n","# These custom image augmentations are needed since we want to perform\n","# transforms such as:\n","# 1. Random horizontal flip\n","# 2. Image resize\n","#\n","# and these operations need to be applied consistently to both the input\n","# image as well as the segmentation mask.\n","class OxfordIIITPetsAugmented(torchvision.datasets.OxfordIIITPet):\n","    def __init__(\n","        self,\n","        root: str,\n","        split: str,\n","        target_types=\"segmentation\",\n","        download=False,\n","        pre_transform=None,\n","        post_transform=None,\n","        pre_target_transform=None,\n","        post_target_transform=None,\n","        common_transform=None,\n","    ):\n","        super().__init__(\n","            root=root,\n","            split=split,\n","            target_types=target_types,\n","            download=download,\n","            transform=pre_transform,\n","            target_transform=pre_target_transform,\n","        )\n","        self.post_transform = post_transform\n","        self.post_target_transform = post_target_transform\n","        self.common_transform = common_transform\n","\n","    def __len__(self):\n","        return super().__len__()\n","\n","    def __getitem__(self, idx):\n","        (input, target) = super().__getitem__(idx)\n","\n","        # Common transforms are performed on both the input and the labels\n","        # by creating a 4 channel image and running the transform on both.\n","        # Then the segmentation mask (4th channel) is separated out.\n","        if self.common_transform is not None:\n","            both = torch.cat([input, target], dim=0)\n","            both = self.common_transform(both)\n","            (input, target) = torch.split(both, 3, dim=0)\n","\n","        if self.post_transform is not None:\n","            input = self.post_transform(input)\n","        if self.post_target_transform is not None:\n","            target = self.post_target_transform(target)\n","\n","        return (input, target)"]},{"cell_type":"code","execution_count":4,"id":"12WkSYwxonMr","metadata":{"id":"12WkSYwxonMr"},"outputs":[],"source":["# Simple torchvision compatible transform to send an input tensor\n","# to a pre-specified device.\n","class ToDevice(torch.nn.Module):\n","    \"\"\"\n","    Sends the input object to the device specified in the\n","    object's constructor by calling .to(device) on the object.\n","    \"\"\"\n","    def __init__(self, device):\n","        super().__init__()\n","        self.device = device\n","\n","    def forward(self, img):\n","        return img.to(self.device)\n","\n","    def __repr__(self) -> str:\n","        return f\"{self.__class__.__name__}(device={device})\""]},{"cell_type":"code","execution_count":5,"id":"P2uKx0B7nCu1","metadata":{"id":"P2uKx0B7nCu1"},"outputs":[],"source":["def save_model_checkpoint(model: nn.Module, path: str) -> None:\n","    print(type(model))\n","    torch.save(model.state_dict(), path)\n","\n","# Load model from saved checkpoint\n","def load_model_from_checkpoint(model: nn.Module, path: str):\n","    return model.load_state_dict(\n","        torch.load(\n","            path,\n","            map_location=get_device(),\n","        )\n","    )\n","\n","def get_model_parameters(m):\n","    total_params = sum(\n","        param.numel() for param in m.parameters()\n","    )\n","    return total_params\n","\n","def print_model_parameters(m):\n","    num_model_parameters = get_model_parameters(m)\n","    print(f\"The Model has {num_model_parameters/1e6:.2f}M parameters\")\n","\n","\n","def get_device() -> torch.device:\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\")\n","    else:\n","        return torch.device(\"cpu\")\n","\n","# Send the Tensor or Model (input argument x) to the right device\n","# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n","# otherwise send to CPU.\n","def to_device(x):\n","    if torch.cuda.is_available():\n","        return x.cuda()\n","    else:\n","        return x.cpu()\n","\n","def close_figures():\n","    while len(plt.get_fignums()) > 0:\n","        plt.close()\n","\n","# Create a tensor for a segmentation trimap.\n","# Input: Float tensor with values in [0.0 .. 1.0]\n","# Output: Long tensor with values in {0, 1, 2}\n","def tensor_trimap(t):\n","    x = t * 255\n","    x = x.to(torch.long)\n","    x = x - 1\n","    return x\n","\n","# Convert a float trimap ({1, 2, 3} / 255.0) into a float tensor with\n","# pixel values in the range 0.0 to 1.0 so that the border pixels\n","# can be properly displayed.\n","def trimap2f(trimap):\n","    return (img2t(trimap) * 255.0 - 1) / 2\n","\n","def args_to_dict(**kwargs):\n","    return kwargs"]},{"cell_type":"markdown","id":"Cz6LQcEy1nhp","metadata":{"id":"Cz6LQcEy1nhp"},"source":["# Data"]},{"cell_type":"markdown","id":"fsmyse4zcT5N","metadata":{"id":"fsmyse4zcT5N"},"source":["*   Wczytaj dane\n","*   Wyświetl jeden obrazek\n","*   Wyświetl jedną etykietę, używając trimap2f oraz t2img\n","\n"]},{"cell_type":"code","execution_count":6,"id":"dPI0SaBtnwGG","metadata":{"id":"dPI0SaBtnwGG"},"outputs":[],"source":["from enum import IntEnum\n","class TrimapClasses(IntEnum):\n","    PET = 0\n","    BACKGROUND = 1\n","    BORDER = 2"]},{"cell_type":"markdown","id":"ScJS8oDEcn5A","metadata":{"id":"ScJS8oDEcn5A"},"source":["\n","\n","*   Stwórz słownik z transformacjami, pamiętaj o zrzutowaniu obrazów do Tensora oraz o zmianie rozmiaru zdjęć, w przypadku etykiet pamiętaj o użyciu funkcji tensor_trimap\n","*   Stwórz zbiór danych pet_train i pet_test z użyciem dostarczonej klasy OxfordIIITPetsAugmented\n","*   Stwórz DataLoadery\n","*   Wydziel ze zbioru treningowego 21 obrazów walidacyjnych\n","*   Sprawdź rozmiary batchów i ich elementów\n","*   Wyświetl jeden batch obrazów ze zbioru treningowego\n","*   Wyświetl jeden batch targetów ze zbioru treningowego\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"My1QLnOPoEzT","metadata":{"id":"My1QLnOPoEzT"},"outputs":[],"source":["transform_dict = args_to_dict(\n","#TODO\n",")\n","\n","# Create the train and test instances of the data loader for the\n","# Oxford IIIT Pets dataset with random augmentations applied.\n","# The images are resized to 128x128 squares, so the aspect ratio\n","# will be chaged. We use the nearest neighbour resizing algorithm\n","# to avoid disturbing the pixel values in the provided segmentation\n","# mask.\n","pets_train = OxfordIIITPetsAugmented(\n","    root='./data',\n","    split='trainval',\n","    pre_transform=transform_dict['pre_transform'],\n","    post_transform=transform_dict['post_transform'],\n","    pre_target_transform=transform_dict['pre_target_transform'],\n","    post_target_transform=transform_dict['post_target_transform']\n",")\n","pets_test = OxfordIIITPetsAugmented(\n","    root='./data',\n","    split='test',\n","    pre_transform=transform_dict['pre_transform'],\n","    post_transform=transform_dict['post_transform'],\n","    pre_target_transform=transform_dict['pre_target_transform'],\n","    post_target_transform=transform_dict['post_target_transform']\n",")\n","\n","pets_train, pets_val = torch.utils.data.random_split(pets_train, #TODO\n",")\n","\n","pets_train_loader = torch.utils.data.DataLoader(\n","#TODO\n","\n",")\n","pets_test_loader = torch.utils.data.DataLoader(\n","#TODO\n","\n",")\n","pets_val_loader = torch.utils.data.DataLoader(\n","#TODO\n","\n",")\n","\n","#TODO\n","train_pets_inputs.shape, train_pets_targets.shape, test_pets_inputs.shape, test_pets_targets.shape, val_pets_inputs.shape, val_pets_targets.shape,"]},{"cell_type":"code","execution_count":null,"id":"f9636d1c-0707-4334-9f7a-3ae655c4b473","metadata":{"id":"f9636d1c-0707-4334-9f7a-3ae655c4b473"},"outputs":[],"source":["# Let's inspect some of the images.\n","pets_input_grid = torchvision.utils.make_grid(\n","    #TODO\n","    )\n","t2img(\n","    #TODO\n","    )"]},{"cell_type":"code","execution_count":null,"id":"zwQUtWy_ojAn","metadata":{"id":"zwQUtWy_ojAn"},"outputs":[],"source":["# Let's inspect the segmentation masks corresponding to the images above.\n","# When plotting the segmentation mask, we want to convert the tensor\n","# into a float tensor with values in the range [0.0 to 1.0]. However, the\n","# mask tensor has the values (0, 1, 2), so we divide by 2.0 to normalize.\n","pets_targets_grid = torchvision.utils.make_grid(\n","    #TODO\n",")\n","t2img(\n","    #TODO\n",")"]},{"cell_type":"code","execution_count":null,"id":"7c36e76a-36a0-4afe-9ac2-4ca1bda659fd","metadata":{"id":"7c36e76a-36a0-4afe-9ac2-4ca1bda659fd"},"outputs":[],"source":["#                  N  C  H\n","# The printed row is the W dimension.\n","train_pets_targets[3][0][4]"]},{"cell_type":"markdown","id":"qsFE4Riz1jiz","metadata":{"id":"qsFE4Riz1jiz"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"id":"f341ce1e-8861-402d-8570-7c6e4265dfaf","metadata":{"id":"f341ce1e-8861-402d-8570-7c6e4265dfaf"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","\n","    def forward(self, x):\n","        out = self.double_conv(x)\n","        return out\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes):\n","        super().__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        self.in_conv = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 1024)\n","        self.up1 = Up(1024, 512)\n","        self.up2 = Up(512, 256)\n","        self.up3 = Up(256, 128)\n","        self.up4 = Up(128, 64)\n","        self.out_conv = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.in_conv(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        return self.out_conv(x)"]},{"cell_type":"markdown","id":"3Kmt9i-t3ht2","metadata":{"id":"3Kmt9i-t3ht2"},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"id":"OdqJK6h_qTC0","metadata":{"id":"OdqJK6h_qTC0"},"outputs":[],"source":["# Define a custom IoU Metric for validating the model.\n","def IoUMetric(pred, gt, softmax=False):\n","    # Run softmax if input is logits.\n","    if softmax is True:\n","        pred = nn.Softmax(dim=1)(pred)\n","\n","    # Add the one-hot encoded masks for all 3 output channels\n","    # (for all the classes) to a tensor named 'gt' (ground truth).\n","    gt = torch.cat([ (gt == i) for i in range(3) ], dim=1)\n","    # print(f\"[2] Pred shape: {pred.shape}, gt shape: {gt.shape}\")\n","\n","    intersection = gt * pred\n","    union = gt + pred - intersection\n","\n","    # Compute the sum over all the dimensions except for the batch dimension.\n","    iou = (intersection.sum(dim=(1, 2, 3)) + 0.001) / (union.sum(dim=(1, 2, 3)) + 0.001)\n","\n","    # Compute the mean over the batch dimension.\n","    return iou.mean()\n","\n","class IoULoss(nn.Module):\n","    def __init__(self, softmax=False):\n","        super().__init__()\n","        self.softmax = softmax\n","\n","    # pred => Predictions (logits, B, 3, H, W)\n","    # gt => Ground Truth Labales (B, 1, H, W)\n","    def forward(self, pred, gt):\n","        # return 1.0 - IoUMetric(pred, gt, self.softmax)\n","        # Compute the negative log loss for stable training.\n","        return -(IoUMetric(pred, gt, self.softmax).log())\n","\n","def prediction_accuracy(ground_truth_labels, predicted_labels):\n","    eq = ground_truth_labels == predicted_labels\n","    return eq.sum().item() / predicted_labels.numel()\n","\n","def test_custom_iou_loss():\n","    #               B, C, H, W\n","    x = torch.rand((2, 3, 2, 2), requires_grad=True)\n","    y = torch.randint(0, 3, (2, 1, 2, 2), dtype=torch.long)\n","    z = IoULoss(softmax=True)(x, y)\n","    return z\n","\n","test_custom_iou_loss()"]},{"cell_type":"code","execution_count":null,"id":"DYpvf9jWowEp","metadata":{"id":"DYpvf9jWowEp"},"outputs":[],"source":["def print_val_dataset_masks(model, val_pets_inputs, val_pets_labels, epoch, save_path, show_plot):\n","    to_device(model.eval())\n","    predictions = model(to_device(val_pets_inputs))\n","    val_pets_labels = to_device(val_pets_labels)\n","\n","    # print(\"Predictions Shape: {}\".format(predictions.shape))\n","    pred = nn.Softmax(dim=1)(predictions)\n","\n","    pred_labels = pred.argmax(dim=1)\n","    # Add a value 1 dimension at dim=1\n","    pred_labels = pred_labels.unsqueeze(1)\n","    # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n","    pred_mask = pred_labels.to(torch.float)\n","\n","    # accuracy = prediction_accuracy(val_pets_labels, pred_labels)\n","    iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n","    iou_accuracy = iou(pred_mask, val_pets_labels)\n","    pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n","    pixel_accuracy = pixel_metric(pred_labels, val_pets_labels)\n","    custom_iou = IoUMetric(pred, val_pets_labels)\n","    title = f'Epoch: {epoch:02d}, Val Accuracy[Pixel: {pixel_accuracy:.4f}, IoU: {iou_accuracy:.4f}, Custom IoU: {custom_iou:.4f}]'\n","    print(title)\n","    # print(f\"Accuracy: {accuracy:.4f}\")\n","\n","    # Close all previously open figures.\n","    close_figures()\n","\n","    fig = plt.figure(figsize=(10, 12))\n","    fig.suptitle(title, fontsize=12)\n","\n","    fig.add_subplot(3, 1, 1)\n","    plt.imshow(t2img(torchvision.utils.make_grid(val_pets_inputs, nrow=7)))\n","    plt.axis('off')\n","    plt.title(\"Input data\")\n","\n","    fig.add_subplot(3, 1, 2)\n","    plt.imshow(t2img(torchvision.utils.make_grid(val_pets_labels.float() / 2.0, nrow=7)))\n","    plt.axis('off')\n","    plt.title(\"Ground Truth Labels\")\n","\n","    fig.add_subplot(3, 1, 3)\n","    plt.imshow(t2img(torchvision.utils.make_grid(pred_mask / 2.0, nrow=7)))\n","    plt.axis('off')\n","    plt.title(\"Predicted Labels\")\n","\n","    if save_path is not None:\n","        plt.savefig(os.path.join(save_path, f\"epoch_{epoch:02}.png\"), format=\"png\", bbox_inches=\"tight\", pad_inches=0.4)\n","\n","    if show_plot is False:\n","        close_figures()\n","    else:\n","        plt.show()"]},{"cell_type":"markdown","id":"fHKa3S-V3lzp","metadata":{"id":"fHKa3S-V3lzp"},"source":["## Run model for testing purposes"]},{"cell_type":"code","execution_count":null,"id":"94PEglWqov_q","metadata":{"id":"94PEglWqov_q"},"outputs":[],"source":["# Run the model once on a single input batch to make sure that the model\n","# runs as expected and returns a tensor with the expected shape.\n","\n","#TODO"]},{"cell_type":"markdown","id":"BD70-sZz3sDQ","metadata":{"id":"BD70-sZz3sDQ"},"source":["## Testing helper functions"]},{"cell_type":"code","execution_count":null,"id":"lrcHpXTDojFz","metadata":{"id":"lrcHpXTDojFz"},"outputs":[],"source":["# Check if our helper functions work as expected and if the image is generated as expected.\n","save_dir = working_dir / \"unet_basic_training_progress_images\"\n","save_dir.mkdir(exist_ok=True)\n","print_val_dataset_masks(\n","    #TODO\n",")"]},{"cell_type":"markdown","id":"h1Y1-INJ2B-5","metadata":{"id":"h1Y1-INJ2B-5"},"source":["# Model training"]},{"cell_type":"code","execution_count":null,"id":"MHp5JFgcowB4","metadata":{"id":"MHp5JFgcowB4"},"outputs":[],"source":["# Train the model for a single epoch\n","def train_model(model, train_data_loader, optimizer):\n","    to_device(model.train())\n","    cel = True\n","    if cel:\n","        criterion = nn.CrossEntropyLoss(reduction='mean')\n","    else:\n","        criterion = IoULoss(softmax=True)\n","\n","    running_loss = 0.0\n","    running_samples = 0\n","\n","    for batch_idx, (inputs, targets) in tqdm(enumerate(train_data_loader, 0)):\n","        optimizer.zero_grad()\n","        inputs = to_device(inputs)\n","        targets = to_device(targets)\n","        outputs = model(inputs)\n","\n","        # The ground truth labels have a channel dimension (NCHW).\n","        # We need to remove it before passing it into\n","        # CrossEntropyLoss so that it has shape (NHW) and each element\n","        # is a value representing the class of the pixel.\n","        if cel:\n","            targets = targets.squeeze(dim=1)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_samples += targets.size(0)\n","        running_loss += loss.item()\n","\n","    print(\"Trained {} samples, Train Loss: {:.4f}\".format(\n","        running_samples,\n","        running_loss / (batch_idx+1),\n","    ))\n","    return running_loss / (batch_idx+1)\n","\n","# Define training loop. This will train the model for multiple epochs.\n","#\n","# epochs: A tuple containing the start epoch (inclusive) and end epoch (exclusive).\n","#         The model is trained for [epoch[0] .. epoch[1]) epochs.\n","#\n","def train_loop(model, train_data_loader, val_data, epochs: Tuple[int, int], optimizer, scheduler, save_path: str, early_stopping_rounds: int = 3) -> None:\n","    val_inputs, val_targets = val_data\n","    epoch_i, epoch_j = epochs\n","    early_stopping_count = 0\n","    best_loss = np.inf\n","    for epoch in range(epoch_i, epoch_j):\n","        print(f\"Epoch: {epoch:02d}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n","        epoch_train_loss = train_model(model, train_data_loader, optimizer)\n","        if epoch_train_loss < best_loss:\n","            best_loss = epoch_train_loss\n","            early_stopping_count = 0\n","        else:\n","            early_stopping_count += 1\n","        with torch.inference_mode():\n","            # Display the plt in the final training epoch.\n","            print_val_dataset_masks(model, val_inputs, val_targets, epoch=epoch, save_path=save_path, show_plot=(epoch == epoch_j-1))\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","        print(f\"Best train loss: {best_loss}\")\n","        print(f\"Current early stop count: {early_stopping_count}\")\n","        print(\"\")\n","        if early_stopping_count > early_stopping_rounds:\n","            print(\"Earlyyyy stopping! :D\")"]},{"cell_type":"code","execution_count":null,"id":"698K58ZbnCKB","metadata":{"id":"698K58ZbnCKB"},"outputs":[],"source":["# Train our model for 20 epochs, and record the following:\n","#\n","# 1. Training Loss\n","# 2. Test accuracy metrics for a single batch (21 images) of test images. The following\n","#    metrics are computed:\n","#   2.1. Pixel Accuracy\n","#   2.2. IoU Accuracy (weighted)\n","#   2.3. Custom IoU Accuracy\n","#\n","# We also plot the following for each of the 21 images in the validation batch:\n","# 1. Input image\n","# 2. Ground truth segmentation mask\n","# 3. Predicted segmentation mask\n","#\n","# so that we can visually inspect the model's progres and determine how well the model\n","# is doing qualitatively. Note that the validation metrics on the set of 21 images in\n","# the validation set is displayed inline in the notebook only for the last training\n","# epoch.\n","\n","# Optimizer and Learning Rate Scheduler.\n","\n","#TODO"]},{"cell_type":"code","execution_count":null,"id":"bp53sNozdhjd","metadata":{"id":"bp53sNozdhjd"},"outputs":[],"source":["# Save the model's checkpoint.\n","\n","# Load both model checkpoints.\n"]},{"cell_type":"markdown","id":"mllAh_QF2Fdc","metadata":{"id":"mllAh_QF2Fdc"},"source":["# Model testing"]},{"cell_type":"code","execution_count":null,"id":"uYg-LSioojDO","metadata":{"id":"uYg-LSioojDO"},"outputs":[],"source":["def test_loop(model, loader):\n","    to_device(model.eval())\n","    iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n","    pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n","\n","    iou_accuracies = []\n","    pixel_accuracies = []\n","    custom_iou_accuracies = []\n","\n","    print_model_parameters(model)\n","\n","    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n","        inputs = to_device(inputs)\n","        targets = to_device(targets)\n","        predictions = model(inputs)\n","\n","        pred_probabilities = nn.Softmax(dim=1)(predictions)\n","        pred_labels = predictions.argmax(dim=1)\n","\n","        # Add a value 1 dimension at dim=1\n","        pred_labels = pred_labels.unsqueeze(1)\n","        # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n","        pred_mask = pred_labels.to(torch.float)\n","\n","        iou_accuracy = iou(pred_mask, targets)\n","        # pixel_accuracy = pixel_metric(pred_mask, targets)\n","        pixel_accuracy = pixel_metric(pred_labels, targets)\n","        custom_iou = IoUMetric(pred_probabilities, targets)\n","        iou_accuracies.append(iou_accuracy.item())\n","        pixel_accuracies.append(pixel_accuracy.item())\n","        custom_iou_accuracies.append(custom_iou.item())\n","\n","        del inputs\n","        del targets\n","        del predictions\n","\n","    iou_tensor = torch.FloatTensor(iou_accuracies)\n","    pixel_tensor = torch.FloatTensor(pixel_accuracies)\n","    custom_iou_tensor = torch.FloatTensor(custom_iou_accuracies)\n","\n","    print(\"Test Dataset Accuracy\")\n","    print(f\"Pixel Accuracy: {pixel_tensor.mean():.4f}, IoU Accuracy: {iou_tensor.mean():.4f}, Custom IoU Accuracy: {custom_iou_tensor.mean():.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"rZmRRLx2rYob","metadata":{"id":"rZmRRLx2rYob"},"outputs":[],"source":["with torch.inference_mode():\n","    # Accuracy of the model. Run test loop\n","    #TODO"]},{"cell_type":"code","execution_count":null,"id":"uhKEQ2WerYtx","metadata":{"id":"uhKEQ2WerYtx"},"outputs":[],"source":["# zip -r unet_basic_training_progress_images.zip  unet_basic_training_progress_images\n","# convert -delay 20 -loop 0 *.png myimage.gif"]},{"cell_type":"markdown","id":"KfHFdr-ea7ka","metadata":{"id":"KfHFdr-ea7ka"},"source":["# Segment Anything 2\n","Ta część zajęć polega na samodzielnym eksperymentowaniu z dokumentacją: https://github.com/facebookresearch/segment-anything\n","\n","Korzystając z notebooków należy dokonać segmentacji na wybranym przez siebie obrazie oraz jednym filmie korzystając zarówno z opcji automatycznej segmentacji jak i używając punktów na obrazie.\n","\n","https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/automatic_mask_generator_example.ipynb\n","\n","https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb\n","\n","https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\n","\n"]},{"cell_type":"code","execution_count":null,"id":"MJM7nXKaAfv4","metadata":{"id":"MJM7nXKaAfv4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
